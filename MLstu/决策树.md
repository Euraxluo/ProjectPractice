# 决策树
优点：计算复杂都不高，输出结果易于理解，对中间值的确是不敏感
缺点：容易过拟合{后期可以使用随机森林}
##决策树剪枝{控制拟合程度}
### 后剪枝
creat tree好了之后，通过计算CT = CT + $$\alpha T_{leaf} $$ 在只通过信息熵建树的基础上引入了叶子节点个数的变量，我们可以通过控制alpha的值，来控制拟合的程度，当alpha大的时候，我们的叶子节点应该少点；当alpha小时，我们的叶子节点可以多点
### 前剪枝
在构造时依据深度或广度限制决策树的拟合

## 创建决策树
	algorithm：CreatBranch():
	检查数据集的分类标签是否相同{或者达到前剪枝条件}
		IF SO ：return 分类标签
		ELSE:
			对数据集悬着信息熵最小的方向分类{信息增益最大}：
			寻找划分数据集最好的特征
			划分数据集
			创建分支节点
	         for 每一个分支：
	         	递归调用CreatBranch()
		return 子节点

## 利用机器学习算法进行数据分析的基本流程
	收集数据：可以使用任何方法
	准备数据：对于数值型数据应该先离散化，标称型数据可以直接使用树构造算法
	数据分析：查看可视化图像{特征工程}
	训练算法：fit过程{构造模型}
	测试算法：使用经验树计算错误率（或者其他CV之内的score判断方法）
	使用算法




















